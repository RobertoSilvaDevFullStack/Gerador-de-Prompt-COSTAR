"""
üöÄ Multi-AI Service OTIMIZADO para produ√ß√£o (Railway)
Vers√£o simplificada e robusta com fallbacks
"""
import os
import json
import httpx
import asyncio
from typing import Dict, Any, Optional
from datetime import datetime
import logging

# Configurar logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ProductionMultiAIService:
    """Servi√ßo Multi-IA otimizado para produ√ß√£o"""
    
    def __init__(self):
        self.providers = self._load_providers()
        self.current_provider = "groq"  # Provider principal
        logger.info(f"üöÄ ProductionMultiAIService inicializado com {len(self.providers)} provedores")
        
    def _load_providers(self) -> Dict[str, Dict[str, Any]]:
        """Carrega provedores dispon√≠veis"""
        providers = {}
        
        # GROQ (Principal - Mais confi√°vel)
        groq_key = os.getenv("GROQ_API_KEY")
        if groq_key:
            providers["groq"] = {
                "name": "Groq",
                "api_key": groq_key,
                "endpoint": "https://api.groq.com/openai/v1/chat/completions",
                "model": "llama-3.1-8b-instant",  # Modelo atual v√°lido
                "priority": 1
            }
            
        # GEMINI (Backup)
        gemini_key = os.getenv("GEMINI_API_KEY")
        if gemini_key:
            providers["gemini"] = {
                "name": "Gemini",
                "api_key": gemini_key,
                "endpoint": "https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent",  # Endpoint atualizado
                "model": "gemini-1.5-flash",  # Modelo atual v√°lido
                "priority": 2
            }
            
        # TOGETHER (Backup 2)
        together_key = os.getenv("TOGETHER_API_KEY")
        if together_key:
            providers["together"] = {
                "name": "Together",
                "api_key": together_key,
                "endpoint": "https://api.together.xyz/v1/chat/completions",
                "model": "meta-llama/Llama-3.2-3B-Instruct-Turbo",  # Modelo atual v√°lido
                "priority": 3
            }
            
        logger.info(f"ü§ñ Provedores carregados: {list(providers.keys())}")
        return providers
    
    async def generate_content(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """Gera conte√∫do usando o melhor provedor dispon√≠vel"""
        
        logger.info(f"üéØ [PROD_AI] Iniciando gera√ß√£o de conte√∫do")
        logger.info(f"üìã [PROD_AI] Provedores dispon√≠veis: {len(self.providers)}")
        logger.info(f"üîë [PROD_AI] Lista de provedores: {list(self.providers.keys())}")
        
        if not self.providers:
            logger.warning("‚ö†Ô∏è [PROD_AI] Nenhum provedor dispon√≠vel, usando fallback")
            return self._fallback_response(prompt)
            
        # Tentar provedores em ordem de prioridade
        for provider_name in sorted(self.providers.keys(), 
                                   key=lambda x: self.providers[x]["priority"]):
            try:
                logger.info(f"üöÄ [PROD_AI] Tentando provedor: {provider_name}")
                logger.info(f"üîë [PROD_AI] API Key presente: {'‚úÖ' if self.providers[provider_name]['api_key'] else '‚ùå'}")
                
                result = await self._try_provider(provider_name, prompt, **kwargs)
                if result:
                    logger.info(f"‚úÖ [PROD_AI] SUCESSO com {provider_name}")
                    logger.info(f"üìè [PROD_AI] Tamanho da resposta: {len(result.get('content', ''))} chars")
                    return result
                else:
                    logger.warning(f"‚ö†Ô∏è [PROD_AI] {provider_name} retornou resultado vazio")
            except Exception as e:
                logger.error(f"‚ùå [PROD_AI] {provider_name} FALHOU: {str(e)}")
                logger.error(f"üîß [PROD_AI] Tipo do erro: {type(e).__name__}")
                continue
        
        # Se todos falharam, usar fallback
        logger.warning("‚ö†Ô∏è [PROD_AI] TODOS os provedores falharam, usando fallback")
        return self._fallback_response(prompt)
        return self._fallback_response(prompt)
    
    async def _try_provider(self, provider_name: str, prompt: str, **kwargs) -> Optional[Dict[str, Any]]:
        """Tenta usar um provedor espec√≠fico"""
        provider = self.providers[provider_name]
        
        if provider_name == "groq":
            return await self._call_groq(provider, prompt, **kwargs)
        elif provider_name == "gemini":
            return await self._call_gemini(provider, prompt, **kwargs)
        elif provider_name == "together":
            return await self._call_together(provider, prompt, **kwargs)
            
        return None
    
    async def _call_groq(self, provider: Dict, prompt: str, **kwargs) -> Dict[str, Any]:
        """Chama API do Groq"""
        try:
            logger.info(f"üåê [GROQ] Iniciando chamada para Groq API")
            logger.info(f"üîó [GROQ] Endpoint: {provider['endpoint']}")
            logger.info(f"ü§ñ [GROQ] Model: {provider['model']}")
            
            headers = {
                "Authorization": f"Bearer {provider['api_key']}",
                "Content-Type": "application/json"
            }
            
            data = {
                "messages": [{"role": "user", "content": prompt}],
                "model": provider["model"],
                "max_tokens": kwargs.get("max_tokens", 1000),
                "temperature": kwargs.get("temperature", 0.7)
            }
            
            logger.info(f"üìä [GROQ] Payload: model={data['model']}, max_tokens={data['max_tokens']}, temp={data['temperature']}")
            
            async with httpx.AsyncClient(timeout=60.0) as client:
                logger.info(f"üì° [GROQ] Enviando requisi√ß√£o...")
                response = await client.post(provider["endpoint"], headers=headers, json=data)
                
                logger.info(f"üì® [GROQ] Status Code: {response.status_code}")
                
                if response.status_code != 200:
                    logger.error(f"‚ùå [GROQ] API erro {response.status_code}: {response.text}")
                    raise Exception(f"HTTP {response.status_code}")
                
                result = response.json()
                logger.info(f"üìã [GROQ] Estrutura da resposta: {list(result.keys()) if isinstance(result, dict) else 'n√£o √© dict'}")
                
                if "choices" not in result or not result["choices"]:
                    logger.error(f"‚ùå [GROQ] Resposta inv√°lida: {result}")
                    raise Exception("Resposta inv√°lida da API")
                
                content = result["choices"][0]["message"]["content"]
                
                logger.info(f"‚úÖ [GROQ] Sucesso - {len(content)} caracteres gerados")
                logger.info(f"üé® [GROQ] Preview: {content[:100]}...")
                
                return {
                    "content": content,
                    "provider": "groq",
                    "model": provider["model"],
                    "success": True
                }
                
        except Exception as e:
            logger.error(f"‚ùå [GROQ] Erro na chamada: {str(e)}")
            logger.error(f"üîß [GROQ] Tipo do erro: {type(e).__name__}")
            raise
    
    async def _call_gemini(self, provider: Dict, prompt: str, **kwargs) -> Dict[str, Any]:
        """Chama API do Gemini"""
        try:
            logger.info(f"üíé [GEMINI] Iniciando chamada para Gemini API")
            logger.info(f"üîó [GEMINI] Endpoint: {provider['endpoint']}")
            logger.info(f"ü§ñ [GEMINI] Model: {provider['model']}")
            
            # Estrutura correta para Gemini v1beta
            data = {
                "contents": [
                    {
                        "parts": [
                            {"text": prompt}
                        ]
                    }
                ],
                "generationConfig": {
                    "maxOutputTokens": kwargs.get("max_tokens", 1000),
                    "temperature": kwargs.get("temperature", 0.7)
                }
            }
            
            # URL com key como par√¢metro
            url = f"{provider['endpoint']}?key={provider['api_key']}"
            
            logger.info(f"üìä [GEMINI] Config: maxTokens={data['generationConfig']['maxOutputTokens']}, temp={data['generationConfig']['temperature']}")
            logger.info(f"üîó [GEMINI] URL: {url}")
            
            async with httpx.AsyncClient(timeout=60.0) as client:
                logger.info(f"üì° [GEMINI] Enviando requisi√ß√£o...")
                response = await client.post(url, json=data)
                
                logger.info(f"üì® [GEMINI] Status Code: {response.status_code}")
                
                if response.status_code != 200:
                    logger.error(f"‚ùå [GEMINI] API erro {response.status_code}: {response.text}")
                    raise Exception(f"HTTP {response.status_code}")
                
                result = response.json()
                logger.info(f"üìã [GEMINI] Estrutura da resposta: {list(result.keys()) if isinstance(result, dict) else 'n√£o √© dict'}")
                
                if "candidates" not in result or not result["candidates"]:
                    logger.error(f"‚ùå [GEMINI] Resposta inv√°lida: {result}")
                    raise Exception("Resposta inv√°lida da API")
                
                content = result["candidates"][0]["content"]["parts"][0]["text"]
                
                logger.info(f"‚úÖ [GEMINI] Sucesso - {len(content)} caracteres gerados")
                logger.info(f"üé® [GEMINI] Preview: {content[:100]}...")
                
                return {
                    "content": content,
                    "provider": "gemini",
                    "model": provider["model"],
                    "success": True
                }
                
        except Exception as e:
            logger.error(f"‚ùå [GEMINI] Erro na chamada: {str(e)}")
            logger.error(f"üîß [GEMINI] Tipo do erro: {type(e).__name__}")
            raise
    
    async def _call_together(self, provider: Dict, prompt: str, **kwargs) -> Dict[str, Any]:
        """Chama API do Together"""
        try:
            logger.info(f"üåê [TOGETHER] Iniciando chamada para Together API")
            logger.info(f"üîó [TOGETHER] Endpoint: {provider['endpoint']}")
            logger.info(f"ü§ñ [TOGETHER] Model: {provider['model']}")
            
            headers = {
                "Authorization": f"Bearer {provider['api_key']}",
                "Content-Type": "application/json"
            }
            
            data = {
                "messages": [{"role": "user", "content": prompt}],
                "model": provider["model"],
                "max_tokens": kwargs.get("max_tokens", 1000),
                "temperature": kwargs.get("temperature", 0.7)
            }
            
            logger.info(f"üìä [TOGETHER] Payload: model={data['model']}, max_tokens={data['max_tokens']}, temp={data['temperature']}")
            
            async with httpx.AsyncClient(timeout=60.0) as client:
                logger.info(f"üì° [TOGETHER] Enviando requisi√ß√£o...")
                response = await client.post(provider["endpoint"], headers=headers, json=data)
                
                logger.info(f"üì® [TOGETHER] Status Code: {response.status_code}")
                
                if response.status_code != 200:
                    logger.error(f"‚ùå [TOGETHER] API erro {response.status_code}: {response.text}")
                    raise Exception(f"HTTP {response.status_code}")
                
                result = response.json()
                logger.info(f"üìã [TOGETHER] Estrutura da resposta: {list(result.keys()) if isinstance(result, dict) else 'n√£o √© dict'}")
                
                if "choices" not in result or not result["choices"]:
                    logger.error(f"‚ùå [TOGETHER] Resposta inv√°lida: {result}")
                    raise Exception("Resposta inv√°lida da API")
                
                content = result["choices"][0]["message"]["content"]
                
                logger.info(f"‚úÖ [TOGETHER] Sucesso - {len(content)} caracteres gerados")
                logger.info(f"üé® [TOGETHER] Preview: {content[:100]}...")
                
                return {
                    "content": content,
                    "provider": "together",
                    "model": provider["model"],
                    "success": True
                }
                
        except Exception as e:
            logger.error(f"‚ùå [TOGETHER] Erro na chamada: {str(e)}")
            logger.error(f"üîß [TOGETHER] Tipo do erro: {type(e).__name__}")
            raise
        
        data = {
            "messages": [{"role": "user", "content": prompt}],
            "model": provider["model"],
            "max_tokens": kwargs.get("max_tokens", 1000),
            "temperature": kwargs.get("temperature", 0.7)
        }
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(provider["endpoint"], headers=headers, json=data)
            response.raise_for_status()
            
            result = response.json()
            content = result["choices"][0]["message"]["content"]
            
            return {
                "content": content,
                "provider": "together",
                "model": provider["model"],
                "success": True
            }
    
    def _fallback_response(self, prompt: str) -> Dict[str, Any]:
        """Resposta de fallback quando todas as IAs falham"""
        logger.warning("üÜò Usando fallback - todas as IAs falharam")
        
        # Resposta b√°sica estruturada
        fallback_content = f"""**PROMPT COSTAR GERADO** (Modo B√°sico)

{prompt}

---
*Nota: Este prompt foi gerado em modo b√°sico. Para an√°lises e melhorias com IA, verifique a configura√ß√£o das APIs.*"""
        
        return {
            "content": fallback_content,
            "provider": "fallback",
            "model": "basic",
            "success": False,
            "message": "APIs de IA indispon√≠veis, usando modo b√°sico"
        }

# Inst√¢ncia global para uso no projeto (lazy loading)
_multi_ai_service = None

def get_multi_ai_service():
    """Obter inst√¢ncia do multi_ai_service com lazy loading"""
    global _multi_ai_service
    if _multi_ai_service is None:
        _multi_ai_service = ProductionMultiAIService()
        logger.info("üöÄ ProductionMultiAIService inicializado (lazy loading)")
    return _multi_ai_service

# Compatibilidade com c√≥digo existente - inicializa√ß√£o no import
try:
    multi_ai_service = get_multi_ai_service()
except Exception as e:
    logger.error(f"‚ùå Erro na inicializa√ß√£o do multi_ai_service: {e}")
    multi_ai_service = None